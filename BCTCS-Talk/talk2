General Recursive Solving Engines

A warm welcome to everyone. I am Arved and this is the current state of my research on general recursive solving engines.

Computer Science has, from the start, been about automation. Never the less, the problem that is still open is: how do we automate computer science, or rather, how to automate automation?
This research focuses on creating better engines and languages for search. Search tasks can be found all over computer science. Searching the right value in a database, searching the solution to a puzzle, the search for the correct weights to a neural network, or even the search for correctly working code; in fact, every problem in computer science eventually turns out to be a search problem.

As Search is such a ubiquitous problem, general purpose search engines have been developed. The probably best known ones are the so called Satisfiability, short SAT-solvers. They are considered the state of the art search engines for boolean logic. They form the core of most bigger search engines like SAT-Modulo-Theory, or SMT-Solvers, First Order Logic sovlers or even exotic engines for, e.g. random SAT. These solving engines however have a few major downsides. The first one is that they have barely any embeddings into state-of-the-art programming languages apart from libraries or bindings. This makes them hard to use, which is a pity, as they could be used for all kinds of tasks like automated programming, type inference, bug hunting...you name them. It should be mentioned that there are languages with solving features like PROLOG and Curry, but their search engines are rather basic compared to state of the art SAT and SMT solvers.

The second downside is that, though SAT-based solving engines perform well in practice, it is very little known as to why that is the case. While some basic principles like unit propagation are straight forward to be proven working under relevant cases, heuristics stay largely subject to practical evaluation. This makes it nearly impossible for current solving systems to improve autonomously, which slows their creation process.

This research aims to solve both of these problems by creating a general recursive programming language that runs as a search engine and additionally allows for proofs on its runtime and memory complexity. This talk will be about the main concept and the proofs of why creating such a language is possible and useful. The implementation of such a language is still ongoing, so it will not be the focus of this talk.


Search and solving are isomorphic tasks, so their names will be used interchangeably. Let's start by looking at why they are such inherit problems of computer science. We will start with a small concrete example of search to then generalise the idea to arbitrary programming languages.


Sudoku is a well known, sometimes hard to crack puzzle. We have a nine by nine grid, where each variable can be assigned a digit between 1 and 9. A valid solution is, when additionally all rows, all columns and all disjoined 3 by 3 squares contain each digit exactly once. A simple brute force algorithm would now test out all combinations of assignments until one represents a solution. Already for a half assigned grid, this would take millions of years to finish. One part of a smarter solution is easy to visualise.

Let's take a smaller grid for a better overview. We fill a 3 by 3 grid with the digits 1 to 3 such that all rows and all columns have each digit exactly once. The idea of the algorithm is simple: If we can directly see that there is only one possibility for a variable assignment, we assign it. This happens when e.g. both its neighbours are assigned.
For an algorithm, each of the constraints now also become objects that we keep track of. These objects observe three variables each. They have two behaviours for how they become active, but we will only look at the simpler one here. In this case, they get active once they realise that two of their variables are assigned. When that happens, they assign the variable and notify all constraints that also observe this variable to check whether they can get active. For those coming from software engineering might feel reminded of the observer pattern. This technique will be named "unit propagation" after the similar technique from SAT-solving. Now, the only thing that needs checking is if a variable is assigned to two conflicting assignments. In that case, there is no solution, as all assignments were without alternative.

In general, unit propagation runs until either there are no more units to propagate or there has been a conflict. A conflict arises if and only if a variable is assigned two conflicting values.

This unit propagation already solves most easy Sudokus. Go ahead, try it out. It does not solve all of them, as we will see later, but the process is fairly simple to implement or model and does quite well in practice.

Now the bigger picture. Why is search such a common problen in computer science?
The grid from the riddle is nothing more than a memory. The indices to the variables become pointers, the variable assignments the contents. We can agree that all programming languages transform such memory. Therefore, any language needs rules to transform the assignment of one part of the memory into another. For this analogy, we generalise the concept of constraints to just be a tuple of variables or pointers together with a predicate that, based on a partial assignment of the variables, can deduce the assignment of other variables.

Imagine this as a function. Some variables are the addresses of the inputs, others are the addresses of the outputs, and from the assignment of the inputs we can deduce the assignment of the outputs. We will go further on this analogy later, but let's for now assume this to be a sufficiently expressible programming language, transforming memory.

This language comes with several upsides. The first one is, that the order in which the values are deduced is not fix. It adjusts to the current state of computation. This is, in fact, quite similar to the lazyness principle of functional and logical languages. As you can imagine, it is quite easy to parallelize. Let each constraint be a thread listening to each other via the variables and the language comes with inherit parallelisation. Furthermore: Remember the function analogy from earlier? If we look at the constraints as functions, we not only get the propagation of deduced values in one direction, we even get the deductions in a backwards direction. This happens every time a certain finiteness is introduced. Each function e.g. is defined by only finitely many clauses, so if only one clause fits the desired output, unit propagation can deduce the input.

In classical functional languages it is extremely hard to reason from the output values to their inputs, even though it could very well be possible using reflection to look into the functions clauses and deduce the reverse function. This researches language brings this feature naturally.

As the proposed type of language has so many upsides, let's further generalise the constraints and values to form a turing complete language. To get an intuition of what needs to be done here: Imagine first that also the constraints have to be somewhere in the memory, and therefore can be reasoned about just as any other value. Therefore, assignments go to arbitrary constraints. Second, there needs to be a mechanism to get more space. In real life programming languages, memory is not modelled as a fix area but rather as an extending address space, so there needs to be a mechanism to allocate new memory.

There are two ways to do so. The easiest is to take any programming language and let its evaluation run on only partially assigned memory, using the unit propagation algorithm from before.
At this point, it should be noted that unit propagation with sufficiently many instantiated variables is just the execution of a program.
Think about it. In a program, the memory containing the program is, well, assigned the program, the input variables are assigned and deterministic languages are designed to run directly deducible once their inputs are fix. Therefore, if we put the program and its input into our memory, unit propagation should do the rest.

To get more concrete, we take something similar to Robin-Elgot's Random Access Machine (RAM). It has four instructions, halting, incrementing a pointer at a given location, copying the content of one pointer into another pointer and jump iff the pointer content is equal. The semantic is now given as a function "step" which defines what happens to the memory and the program counter when a certain instruction is executed. The unit propagation is now a function that takes a memory and a list of still active constraints, represented by a list of pointers to the instruction in the memory, to deduce the next memory state and the next list of active constraints. In an actual implementation, all of this would most likely be wrapped in the IO monad to use its direct memory interface.

It should be noted here that there is a slight complication with one of the instructions modifying memory. When using unit propagation, memory needs to be immutable. There is a workaround to this, but it will not be discussed in this talk. Easiest solution is to use a functional language core.

This raw RAM machine is kinda bulky to make an actual language, but it has few instructions, easing the implementation. One feature however, that would greatly enhance readability of the language is, that, as mentioned before, a variable should not just hold a pointer, but an arbitrary instruction or constraint. This eases any kind of higher order semantics. This is a sketch how a low level language that has this higher order feature could look like. There is an instruction "assign-new" that creates a new constraint with a given constructor at a given location. As each constraint is a sort of tuple filled with pointers, all pointers now point to new locations. To access these pointers and set them to a desired value, there are accessor functions provided. Please note how this would need a small type system to work, we just assumes unit propagation derives a conflict in case a non existing value is accessed.

Implementation details aside: No matter what base language is chosen, with this technique we can already create languages with stronger search properties than PROLOG! Try it out. Both depth first and breadth first search have examples where they are slow at that could have been fast under unit propagation.
Also, the construction of just taking any language and interrupting its computation when the inputs to the instructions have not been fully instantiated works for any programming language, so we get the theorem that

We now created a programming language that already has stronger search features than classical logical languages like PROLOG. Please also observe that this turned out to be quite easy. Just take an existing programming languages and allow it to stop its computation when needed values have not yet been assigned, treating its instructions like constraints. This results in a theorem that every general recursive language can be transformed into an equaly general recursive unit propagating search engine. Asterisk: If the language is enhanced to be able to express free variables. This is the first result of this research.

This can now be further enhanced. If unit propagation were to suffice for good search, whole branches of theoretical computer science would not need to exist. So what about the cases where unit propagation does not succeed?

When unit propagation terminates without conflict, some variables could remain unassigned, while they still have constraints on them. This can be useful when there are several possible solutions. The dangling search state is a representation of the solution space. It however might be quite a bad representation as, in the worst case, it is just the problem that we started with. Therefore, a good way for further improvement would be evaluate the term further such that querying for solutions becomes faster. If the querying uses an algorithm running on unit propagation, we can make complexity analysis on it! Good thing we have a formal representation of our language.
Thinking this aim one step further, it would be cool to have an algorithm that does the whole solving using only unit propagation. Can unit propagation be that complete? Yes, very much so.

A partial memory state S is a representation of partially assigned memory, and a few extras. It is just a way to have a memory state placed in our memory. Now there is a term T with some input pointer that we give this partial memory state to, such that this memory state, together with T, unit propagates to some assignment. Such a term T is called a solver and we already know those exist.

We can do something similar by creating a term U that essentially just mimics unit propagation using unit propagation. The upside of such a term is that now we can deduce some meta-properties, like the number of deductions made during the propagation or the amount of memory used. This can be used to evaluate our solver.

What's furthermore possible: With this simulated unit propagation, we can leave the solver unassigned. What happens now? Well, the solver itself is being deduced through the backwards propagation. Sure, not everything might be deducible, but that's what we have interaction for, right?

The amazing thing is that now, we can do something that could revolutionize interactive theorem proving. Here we see, that it is totally allowed to use the solver we just deduced to also deduce its own deduction. Let that sink in for a second. As in the unit propagation we can do circular deductions, the solver that we deduce somewhere can be directly used to further deduce properties about itself. Imagine having an interactive theorem prover that immediately uses any statement on search that you deduce to speed up any further search. The longer we'd use that system the faster and more autonomous it's get, and many more other implications that we don't have time for now. But it is an idea that I hope more computer scientists will hop onto to help me develop it further.

To summarise the current results:
Every programming language can be turned into a search engine.
Unit propagation is a form of evaluation that comes with many upsides.
Using the nature of cyclic deductions, search engines could be automatically created.

As of my personal state in implementing the language: One current aim is to create a verified implementation using AGDA. So far, a memory interface has been formalised that makes it possible to create the proofs around memory. The long term aim is to proof the concept works in AGDA and then extract a C-like implementation of the language from that proof. And maybe, while doing so, proof the existence of the unit propagation and solving term in the unit propagating language to get the automated solver creation rolling.

But most important takeaway:
The concept is ready.
We just need to build it.

Thank you for your attention. Please ask questions.
